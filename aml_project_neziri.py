# -*- coding: utf-8 -*-
"""AML Project Neziri.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N-qBPaPqtIzemP1oD-_cHJSqjupSMMju
"""

import pandas as pd
import numpy as np
np.random.seed(7)
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from keras.models import Model
from keras.layers import Dense, Input
from keras.regularizers import l1,l2
from keras.optimizers import Adam

#Load Data

from pandas import read_csv

train_file = 'train_imperson_without4n7_balanced_data.csv'
test_file = 'test_imperson_without4n7_balanced_data.csv'

train = read_csv(train_file)
test = read_csv(test_file)

labels = train.columns
x_labels = train.columns.values[0:train.values.shape[1]-1]
x_labels = [int(i) for i in x_labels]
 
array = train.values
x_train = array[:,0:array.shape[1]-1]
y_train = array[:,array.shape[1]-1]

test_array = test.values
x_test = test_array[:,0:test_array.shape[1]-1]
y_test = test_array[:,test_array.shape[1]-1]

x_size = x_train.shape[1]

print(x_train.shape)
print(x_test.shape)
print(x_labels)

#Stacked Autoencoder

input_size = x_size
hidden_size = 100
code_size = 50

input_data = Input(shape=(input_size,))
encoder1 = Dense(hidden_size, activation='relu')(input_data)
encoder2 = Dense(code_size, activation='relu')(encoder1)
decoder1 = Dense(hidden_size, activation='relu')(encoder2)
decoder2 = Dense(input_size, activation='sigmoid')(decoder1)

autoencoder = Model(input_data, decoder2)
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')
autoencoder.fit(x_train, x_train, epochs=5, validation_data=(x_test,x_test))

#Combine SAE output witt original dataset

encoder = Model(input_data,encoder2)
latent_vector = encoder.predict(x_test)

#Encode the datasets to create combined dataset

encode_x_train = encoder.predict(x_train)
encode_x_test = encoder.predict(x_test)

#Add created features to original dataset

x_train_large = np.concatenate((x_train, encode_x_train),axis=1)
x_test_large = np.concatenate((x_test,encode_x_test),axis=1)
x_labels_sae = list(range(155,205))
x_labels_large = x_labels + x_labels_sae

print(x_test_large.shape)
print(x_train_large.shape)
print(x_labels_large)

#Feature selection: Mutual Information (MI)

from sklearn.feature_selection import mutual_info_classif
model_mi = mutual_info_classif(x_train_large,y_train)

Features_selected = 5

top_5_train_mi = x_train_large[:,model_mi.argsort()[-Features_selected:]]
top_5_test_mi = x_test_large[:,model_mi.argsort()[-Features_selected:]]

non_zero_mi = []
for i in range(0,len(model_mi)):
  if model_mi[i] != 0:
    non_zero_mi.append((x_labels_large[i],model_mi[i]))

top_5_train_mi_labels = sorted([(x,i) for (i,x) in non_zero_mi],reverse=True)[:5]

print(non_zero_mi)
print(len(non_zero_mi))
print(top_5_train_mi_labels)

#Feature selection: Feature Importance using Extra Trees (FI)

from sklearn.ensemble import ExtraTreesClassifier

model_fi = ExtraTreesClassifier()
model_fi_fit = model_fi.fit(x_train_large, y_train)
model_fi_values = model_fi_fit.feature_importances_

Features_selected = 5

top_5_train_fi = x_train_large[:,model_fi_values.argsort()[-Features_selected:]]
top_5_test_fi = x_test_large[:,model_fi_values.argsort()[-Features_selected:]]

non_zero_fi = []
for i in range(0,len(model_fi_values)):
  if model_fi_values[i] != 0:
    non_zero_fi.append((x_labels_large[i], model_fi_values[i]))

top_5_train_fi_labels = sorted([(x,i) for (i,x) in non_zero_fi],reverse=True)[:5]

print(non_zero_fi)
print(len(non_zero_fi))
print(top_5_train_fi_labels)

#Feature Selection: Recursive Feature Elimination (RFE)

from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

model_rfe = LogisticRegression()
rfe = RFE(model_rfe, n_features_to_select=5, step=5)
model_rfe_fit = rfe.fit(x_train_large, y_train)

top_5_train_rfe =  x_train_large[:,rfe.get_support(1)]
top_5_test_rfe =  x_test_large[:,rfe.get_support(1)]

print('RFE feature values: ','\n',rfe.support_,'\n')
print('RFE feature ranking: ','\n',rfe.ranking_,'\n')

rfe_index = rfe.get_support(1)

top_5_train_rfe_labels = []

for index in rfe_index:
  top_5_train_rfe_labels.append(x_labels_large[index])

print('RFE top 5 features: ','\n',top_5_train_rfe_labels)

#MLP Neural Network

mlp_layer_size = 5

def mlp_model(training_data, testing_data):
  mlp = Sequential()
  mlp.add(Dense(training_data.shape[1], input_dim=training_data.shape[1], activation='relu'))
  mlp.add(Dense(mlp_layer_size, activation='relu'))
  mlp.add(Dense(1, activation='sigmoid'))
  mlp.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
  mlp.fit(training_data, y_train, validation_data=(testing_data,y_test), epochs=150, batch_size=500)
  return mlp

#Tune Models
from scipy.stats import loguniform, uniform, randint
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV 
from keras.models import Sequential

np.random.seed(7)

model1 = LogisticRegression()
model2 = DecisionTreeClassifier()

#Grid Search Function

def gridsearch(model,param_grid,x_data):
  grid = GridSearchCV(estimator=model,param_grid=param_grid)
  grid.fit(x_data,y_train)
  print(grid.best_estimator_)
  print(grid.best_score_)

#LR
param_grid1 = {'C': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100],
               'penalty': ['l1', 'l2'],
               'solver': ['liblinear', 'saga']}

#CART
param_grid2 = {'criterion':['gini','entropy'],
               'max_depth':np.arange(1,10),
               'min_samples_split':np.arange(1,10),
               'min_samples_leaf':np.arange(1,5)}              

#MLP

param_grid3 = {'optimizer': ['sgd', 'adam'],
               'epochs': [50, 100, 150],
               'batch_size': [100, 250, 500]
               }


#Random Search Function

def randomsearch(model,param_dist,x_data):
  rsearch = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=100, random_state=7)
  rsearch.fit(x_data,y_train)
  print(rsearch.best_estimator_)
  print(rsearch.best_score_)

#LR

param_dist1 = {'C': loguniform(0.00001,100),
               'penalty': ['l1', 'l2'],
               'solver': ['liblinear', 'saga']}

#CART

param_dist2 = {'criterion':['gini','entropy'],
               'max_depth': randint(1,10),
               'min_samples_split': randint(1,10),
               'min_samples_leaf': randint(1,5)} 

#MLP

param_dist3 = {'optimizer': ['sgd', 'adam'],
               'epochs': randint(50,150),
               'batch_size': randint(100,500)}

#Table 1 - Logistic Regression tuning with different data selection techniques

#Grid Search

print('fi')
gridsearch(model1,param_grid1,top_5_train_fi)
print('mi')
gridsearch(model1,param_grid1,top_5_train_mi)
print('rfe')
gridsearch(model1,param_grid1,top_5_train_rfe)

#Random Search

print('fi')
randomsearch(model1,param_dist1,top_5_train_fi)
print('mi')
randomsearch(model1,param_dist1,top_5_train_mi)
print('rfe')
randomsearch(model1,param_dist1,top_5_train_rfe)

#Table 2 - CART tuning with different data selection techniques

#Grid Search

print('fi')
gridsearch(model2,param_grid2,top_5_train_fi)
print('mi')
gridsearch(model2,param_grid2,top_5_train_mi)
print('rfe')
gridsearch(model2,param_grid2,top_5_train_rfe)

#Random Search

print('fi')
randomsearch(model2,param_dist2,top_5_train_fi)
print('mi')
randomsearch(model2,param_dist2,top_5_train_mi)
print('rfe')
randomsearch(model2,param_dist2,top_5_train_rfe)

#Table 3 - MLP tuning with different data selection techniques

from keras.wrappers.scikit_learn import KerasClassifier

def mlp_tune_model(optimizer='adam'):
  mlp_tune = Sequential()
  mlp_tune.add(Dense(5, input_dim=5, activation='relu'))
  mlp_tune.add(Dense(5, activation='relu'))
  mlp_tune.add(Dense(1, activation='sigmoid'))
  mlp_tune.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
  return mlp_tune

model_mlp = KerasClassifier(build_fn=mlp_tune_model, verbose=0)


#Grid Search

gridsearch_mlp = GridSearchCV(estimator=model_mlp, param_grid=param_grid3, n_jobs=-1, cv=3)

print('fi')
gs_mlp_fi = gridsearch_mlp.fit(top_5_train_fi,y_train)
print(gs_mlp_fi.best_params_)
print(gs_mlp_fi.best_score_)

print('mi')
gs_mlp_mi = gridsearch_mlp.fit(top_5_train_mi,y_train)
print(gs_mlp_mi.best_params_)
print(gs_mlp_mi.best_score_)

print('rfe')
gs_mlp_rfe = gridsearch_mlp.fit(top_5_train_rfe,y_train)
print(gs_mlp_rfe.best_params_)
print(gs_mlp_rfe.best_score_)

#Random Search

rsearch_mlp = RandomizedSearchCV(estimator=model_mlp, param_distributions=param_dist3, n_jobs=-1, cv=3)

print('fi')
rs_mlp_fi = rsearch_mlp.fit(top_5_train_fi,y_train)
print(rs_mlp_fi.best_params_)
print(rs_mlp_fi.best_score_)

print('mi')
rs_mlp_mi = rsearch_mlp.fit(top_5_train_mi,y_train)
print(rs_mlp_mi.best_params_)
print(rs_mlp_mi.best_score_)

print('rfe')
rs_mlp_rfe = rsearch_mlp.fit(top_5_train_rfe,y_train)
print(rs_mlp_rfe.best_params_)
print(rs_mlp_rfe.best_score_)

#Evaluation metrics
#Recall score = Detection Rate (DR)

from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import matthews_corrcoef
from sklearn.metrics import precision_score

def evaluation_metrics(y_true,y_pred):

  cf = confusion_matrix(y_true,y_pred)
  tn = cf[0,0]
  fn = cf[1,0]
  fp = cf[0,1]
  tp = cf[1,1]

  false_alarm_rate = fp / (tn + fp)
  false_negative_rate = fn / (fn + tp)

  print('Confusion Matrix: ','\n')
  print(confusion_matrix(y_true,y_pred),'\n')
  print('Accuracy: ', accuracy_score(y_true,y_pred))
  print('Detection Rate: ', recall_score(y_true,y_pred))
  print('F1: ', f1_score(y_true,y_pred))
  print('MCC: ', matthews_corrcoef(y_true,y_pred))
  print('Precision: ', precision_score(y_true,y_pred))
  print('FAR: ', false_alarm_rate)
  print('FNR: ', false_negative_rate)

#Evaluate Models

from sklearn.model_selection import KFold
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression

models = []
models.append(('LR', LogisticRegression()))
models.append(('CART', DecisionTreeClassifier()))

x_labels = ['Training data plus SAE output','MI Selected Features','FI Selected Features','RFE Selected Features']
x_train_set = [x_train_large, top_5_train_mi, top_5_train_fi, top_5_train_rfe]
x_test_set = [x_test_large, top_5_test_mi, top_5_test_fi, top_5_test_rfe]

for name, model in models:
  print('\n',name)
  for i in range(0,len(x_train_set)):
    print('\n',x_labels[i],'\n')
    model.fit(x_train_set[i], y_train)
    y_pred  = model.predict(x_test_set[i])
    evaluation_metrics(y_test,y_pred)

#MLP Evalutaion Metrics

print('MLP')

for i in range(0,len(x_train_set)):
  print('\n',x_labels[i],'\n')
  mlp = mlp_model(x_train_set[i],x_test_set[i])
  mlp_y_predict = mlp.predict(x_test_set[i])
  mlp_y_classes = mlp.predict_classes(x_test_set[i])
  evaluation_metrics(y_test, mlp_y_classes)